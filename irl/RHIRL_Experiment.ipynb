{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RHIRL [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import defaultdict\n",
    "\n",
    "from simple_rl.tasks import NavigationMDP\n",
    "from simple_rl.agents import QLearningAgent\n",
    "from simple_rl.planning import ValueIteration\n",
    "from simple_rl.tasks.grid_world.GridWorldStateClass import GridWorldState\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)\n",
    "nvmdp = NavigationMDP(width=30, \n",
    "                        height=30, \n",
    "                        goal_locs=[(21,21)], \n",
    "                        init_loc=(1,1), \n",
    "                        rand_init=True,\n",
    "                        cell_types=[\"empty\", \"yellow\", \"red\", \"green\", \"purple\"],\n",
    "                        cell_type_rewards=[0, 0, -10, -10, -10],\n",
    "                        goal_reward=1.,\n",
    "                        slip_prob=0.00,\n",
    "                        step_cost=0.0,\n",
    "                        gamma=.99)\n",
    "\n",
    "value_iter = ValueIteration(nvmdp, sample_rate=100)\n",
    "_ = value_iter.run_vi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvmdp.visualize_grid(nvmdp.cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data(nvmdp, n_trajectory, phi):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        n_trajectory: number of trajectories to sample\n",
    "\n",
    "    Returns:\n",
    "        [[phi(s1), phi(s3), ...], [phi(s1), phi(s3), ...], ...], where phi is one-hot encoded vector \n",
    "                of the cell type of a given state  \n",
    "    \"\"\"\n",
    "    A_s = []\n",
    "    D_mdp_states = []\n",
    "    \n",
    "    action_to_idx = {a:i for i,a in enumerate(nvmdp.actions)}\n",
    "    \n",
    "    for _ in range(n_trajectory):\n",
    "        action_seq, state_seq = value_iter.plan(nvmdp.get_random_init_state())\n",
    "        #print(len(action_seq), len(state_seq))\n",
    "        D_mdp_states.append(state_seq)\n",
    "        A_s.append([action_to_idx[a] for a in action_seq])\n",
    "    return D_mdp_states, A_s\n",
    "\n",
    "def feature_long_horizon(nvmdp, x, y):\n",
    "    \n",
    "    row,col = nvmdp._xy_to_rowcol(x,y)\n",
    "    if (x, y) in nvmdp.goal_locs:\n",
    "        return np.zeros(len(nvmdp.cell_types), dtype=np.float32)\n",
    "    else:\n",
    "        return np.eye(len(nvmdp.cell_types))[nvmdp.cells[row, col]]\n",
    "    \n",
    "def feature_short_horizon(nvmdp, x, y):\n",
    "    row,col = nvmdp._xy_to_rowcol(x,y)\n",
    "    return np.hstack((feature_long_horizon(nvmdp, x, y), nvmdp.feature_cell_dist[row, col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "N_tau = 8\n",
    "phi_long=lambda mdp_state: feature_long_horizon(nvmdp, mdp_state.x, mdp_state.y)\n",
    "phi_short=lambda mdp_state: feature_short_horizon(nvmdp, mdp_state.x, mdp_state.y)\n",
    "D_mdp_states, A_s = sample_data(nvmdp, N_tau, phi=phi_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvmdp.visualize_grid(trajectories=D_mdp_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def states_to_features(states, phi):\n",
    "    return np.asarray([phi(s) for s in states], dtype=np.float32)\n",
    "\n",
    "def get_h_transitions_heap(state, phi, T, actions, h):\n",
    "    \n",
    "    frontier = [(state, 0, 0)] # for i, a in enumerate(range(len(actions)))] # BFS frontier: [(state, heap_idx, depth), ...]\n",
    "    heap = [state] # for a in range(len(actions))]\n",
    "    parents = [None]\n",
    "    parent_idx = [-1]\n",
    "    depths = [0] # for a in range(len(actions))]\n",
    "    idx = 0\n",
    "    \n",
    "    while len(frontier) != 0:\n",
    "        \n",
    "        state, heap_idx, d = frontier.pop(0)\n",
    "        \n",
    "        if d >= h:\n",
    "            break\n",
    "            \n",
    "        for a in actions:\n",
    "            state_prime = T(state, a)\n",
    "            frontier.append((state_prime, idx+1, d+1))\n",
    "            heap.append(state_prime)\n",
    "            depths.append(d+1)\n",
    "            parents.append(state)\n",
    "            parent_idx.append(heap_idx)\n",
    "            idx += 1\n",
    "            \n",
    "    return states_to_features(heap, phi), np.array(depths, np.int32), parents, np.asarray(parent_idx)\n",
    "\n",
    "def get_training_data(state, phi, T, actions, h):\n",
    "    \n",
    "    state_feature_tree, state_depths, _, parent_idx = get_h_transitions_heap(state, phi, T, actions, h)\n",
    "    next_state_marker = np.asarray([parent_idx==i for i in range(len(state_feature_tree))], dtype=np.float32)\n",
    "    tf_sft = np.array([np.where(state_depths.reshape(-1,1) == i, state_feature_tree, np.zeros_like(state_feature_tree)) for i in range(h)][::-1])\n",
    "    return tf_sft, state_feature_tree, state_depths, next_state_marker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9999\n",
    "h = 6\n",
    "nA = len(nvmdp.actions)\n",
    "phi = phi_short if h < 45 else phi_long\n",
    "phi_s_dim = len(phi(GridWorldState(1,1)))\n",
    "sft, state_feature_tree, state_depths, next_state_marker = get_training_data(D_mdp_states[0][0], phi, nvmdp.transition_func, nvmdp.actions, h)\n",
    "heap_size = len(state_depths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF Static Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "\"\"\"\n",
    "RHIRL TF v4\n",
    "\"\"\"\n",
    "def ComputeValue(W_r, V, Q, Pi, V_pad, sft, i):\n",
    "    \n",
    "    Phi_S = tf.gather(sft, i) # For i=0, this should give h-1 step features\n",
    "    R = tf.matmul(Phi_S, W_r, name=\"R\")\n",
    "    V_prev = gamma * tf.expand_dims(V.read(i, name=\"V_prev\"), -1) # For i=0, this will read hth step values which are 0\n",
    "    Q_h = tf.squeeze(tf.add(R, tf.multiply(gamma, V_prev), name=\"Q\"))\n",
    "    Q_h = tf.reshape(Q_h[1:], (-1, nA), \"Q_sa_reshaped\")\n",
    "    Pi_h = tf.nn.softmax(Q_h, axis=-1)\n",
    "    V_h = tf.reduce_sum(tf.multiply(Q_h, Pi_h), axis=-1, name=\"V\")\n",
    "    V_h = tf.squeeze(tf.pad(tf.reshape(V_h, [-1,1]), [[0, V_pad], [0, 0]], 'CONSTANT'))\n",
    "    V = V.write(tf.add(i,1), V_h, name=\"V_update\")\n",
    "    Q = Q.write(tf.add(i,1), Q_h, name=\"Q_update\")\n",
    "    Pi = Pi.write(tf.add(i,1), Pi_h, name=\"Pi_update\")\n",
    "    i = tf.add(i, 1)\n",
    "    return W_r, V, Q, Pi, V_pad, sft, i\n",
    "\n",
    "def run_RHC(W_r, sft, h):\n",
    "    \n",
    "    V = tf.TensorArray(tf.float32, size=0, dynamic_size=True,\n",
    "                             clear_after_read=False, infer_shape=False, name=\"V_array\")\n",
    "    V = V.write(0,  np.zeros(heap_size, dtype=np.float32), name=\"V_array_0\")\n",
    "    Q = tf.TensorArray(tf.float32, size=0, dynamic_size=True,\n",
    "                                 clear_after_read=False, infer_shape=False, name=\"Q_array\")\n",
    "    Q = Q.write(0,  np.zeros((int(heap_size//nA), nA), dtype=np.float32), name=\"Q_array_0\")\n",
    "    Pi = tf.TensorArray(tf.float32, size=0, dynamic_size=True,\n",
    "                                 clear_after_read=False, infer_shape=False, name=\"Pi_array\")\n",
    "    Pi = Pi.write(0,  np.ones((int(heap_size//nA), nA), dtype=np.float32)/nA, name=\"Pi_array_0\")\n",
    "    V_pad = tf.constant( int(( nA * (1-nA**(h))/(1-nA) ) - ( nA * (1-nA**(h-1))/(1-nA) )), dtype=tf.int32)\n",
    "\n",
    "    loop_cond = lambda W_r, V, Q, Pi, V_pad, sft, i: tf.less(i, h-1, name=\"compute_value_end\")\n",
    "    W_r, V, Q, Pi, V_pad, sft, i = tf.while_loop(loop_cond, ComputeValue, [W_r, V, Q, Pi, V_pad, sft, 0], parallel_iterations=1, name=\"compute_value_loop\")\n",
    "    return V, Q, Pi\n",
    "\n",
    "tf.set_random_seed(0)\n",
    "tf_sft = tf.placeholder(tf.float32, name=\"state_feature_tree\")\n",
    "action_idx = tf.placeholder(tf.int32, name=\"action_idx\")\n",
    "lr = tf.placeholder(tf.float32, name=\"learning_rate\")\n",
    "# W_r = tf.get_variable(\"W_r\", dtype=tf.float32, initializer=tf.fill((phi_s_dim, 1), 0.)) # good prior\n",
    "W_r = tf.get_variable(\"W_r\", dtype=tf.float32, initializer=tf.random_uniform((phi_s_dim, 1), 0, 0.1)) # better?\n",
    "V, Q, Pi = run_RHC(W_r, tf_sft, h)\n",
    "log_likelihood = -tf.log(Pi.read(h-1)[0, action_idx])\n",
    "W_grad = tf.gradients(log_likelihood, W_r)\n",
    "sgd = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "adam = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "updateGrads = adam.apply_gradients(zip(W_grad, [W_r]))\n",
    "prob_summary = tf.summary.scalar('P_s_a', Pi.read(h-1)[0, action_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check gradients\n",
    "Try to overfit the model: provide single sample and optimize over Pi(s_0, a_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    state_feature_tree, _, __, ___ = get_training_data(D_mdp_states[0][0], phi, nvmdp.transition_func, nvmdp.actions, h)\n",
    "    feed_dict = {tf_sft: sft, action_idx: 0, lr:0.5}\n",
    "    p_vals = []\n",
    "    for i in range(10):\n",
    "        v, q, pi, grad, ugrad = sess.run(fetches=[V.stack(), Q.stack(), Pi.stack(), W_grad, updateGrads], feed_dict=feed_dict)\n",
    "        p_vals.append(pi[-1, 0, 0])\n",
    "        # print(p_vals[-1])\n",
    "        \n",
    "plt.plot(p_vals), plt.title(\"p(s_0,a)\"), plt.xlabel(\"iter\"), plt.ylabel(\"P\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RHIRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cell_rewards(nvmdp, w_r):\n",
    "    r_map = np.zeros((nvmdp.height, nvmdp.width), dtype=np.float32)\n",
    "    for row in range(nvmdp.height):\n",
    "        for col in range(nvmdp.width):\n",
    "            x, y = nvmdp._rowcol_to_xy(row, col)\n",
    "            r_map[row, col] = feature_short_horizon(nvmdp, x, y).dot(w)[0]\n",
    "    return r_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    merged_summary = tf.summary.merge([prob_summary])\n",
    "    p_vals = defaultdict(lambda: [])\n",
    "    \n",
    "    cnt = 0\n",
    "    for i in range(10):\n",
    "        \n",
    "        print(\"Epoch {}:\".format(i))\n",
    "        for t_i, traj in enumerate(D_mdp_states):\n",
    "\n",
    "            train_writer = tf.summary.FileWriter(\"./rhirl/summaries/train\", sess.graph)\n",
    "            first_state_key = str(traj[0]) + \", a: \" + str(A_s[t_i][0])\n",
    "            for s_i, state in enumerate(traj[:-1]): # in last state no action is taken, so ignore\n",
    "\n",
    "                sft, _, __, ___ = get_training_data(state, phi, nvmdp.transition_func, nvmdp.actions, h)\n",
    "                feed_dict = {tf_sft: sft, action_idx: A_s[t_i][s_i], lr: 0.1}\n",
    "                v, q, pi, w, grad, ugrad, summary = sess.run(\n",
    "                            fetches=[V.stack(), Q.stack(), Pi.stack(), W_r, W_grad, updateGrads, merged_summary], feed_dict=feed_dict)\n",
    "                p_vals[first_state_key].append(pi[-1, 0, A_s[t_i][0]])\n",
    "            train_writer.add_summary(summary, cnt)\n",
    "            train_writer.close()\n",
    "            cnt += 1\n",
    "            # Note: this only prints P(first state, first action) of each trajectory \n",
    "            print(\"\\t P({}, a: {}): {}\".format(traj[0], A_s[t_i][s_i], p_vals[first_state_key][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in p_vals.items():\n",
    "    plt.plot(v, label=k)\n",
    "    plt.xlabel(\"iter\")\n",
    "    plt.ylabel(\"P\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_rewards = compute_cell_rewards(nvmdp, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"cell, true, obtained\")\n",
    "list(zip([\"empty\", \"yellow\", \"red\", \"green\", \"purple\"],[0, 0, -10, -10, -10], [round(n,2) for n in np.eye(len(nvmdp.cell_types)*2).dot(w)[:,0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recovered Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,12)) \n",
    "# cmap=colors.ListedColormap(['white','yellow','red','green','purple', 'blue'])\n",
    "nvmdp.visualize_grid(nvmdp.cells, trajectories=D_mdp_states, \n",
    "                     subplot_str=\"131\", new_fig=False)\n",
    "nvmdp.visualize_grid(nvmdp.cell_rewards, trajectories=D_mdp_states, \n",
    "               cmap=plt.cm.gray, subplot_str=\"132\", title=\"True Rewards\", new_fig=False)\n",
    "nvmdp.visualize_grid(cell_rewards, trajectories=D_mdp_states, \n",
    "               cmap=plt.cm.gray, subplot_str=\"133\", new_fig=False, title=\"RHIRL: Recovered Rewards (h={})\".format(h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "[1] MacGlashan, James, and Michael L. Littman. \"Between Imitation and Intention Learning.\" IJCAI. 2015.\n",
    "\n",
    "## Credits\n",
    "Prof. Michael Littman, Monica Vroman, and Jun Ki Lee"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "littman_irl_python3",
   "language": "python",
   "name": "littman_irl_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
