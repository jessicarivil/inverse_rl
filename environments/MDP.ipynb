{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "import mdptoolbox\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDPToolBoxWrapper:\n",
    "    \"\"\"Convert the plain object description of the mdp into gamma and T and R matrices\"\"\"\n",
    "    def __init__(self, descr):\n",
    "        self.descr = descr\n",
    "        self.gamma = descr[\"gamma\"]\n",
    "        self.nS = len(descr[\"states\"])\n",
    "        self.nA = len(descr[\"states\"][0][\"actions\"])\n",
    "        self.transitions = np.zeros((self.nA, self.nS, self.nS))\n",
    "        self.rewards = np.zeros((self.nA, self.nS, self.nS))\n",
    "        state_indexes = {state[\"id\"]: i for i, state in enumerate(descr[\"states\"])}\n",
    "        for state in descr[\"states\"]:\n",
    "            assert len(state[\"actions\"]) == self.nA, \"All states must have same number of possible actions\"\n",
    "            for i, action in enumerate(state[\"actions\"]):\n",
    "                for transition in action[\"transitions\"]:\n",
    "                    state_index = state_indexes[state[\"id\"]]\n",
    "                    new_state_index = state_indexes[transition[\"to\"]]\n",
    "                    self.transitions[i, state_index, new_state_index] = transition[\"probability\"]\n",
    "                    self.rewards[i, state_index, new_state_index] = transition[\"reward\"]\n",
    "                    \n",
    "def create_MDP(P, gamma):\n",
    "    \n",
    "    mdp = {}\n",
    "    mdp[\"gamma\"] = gamma\n",
    "    mdp[\"states\"] = []\n",
    "        \n",
    "    for s in P:\n",
    "        mdp[\"states\"].append({})\n",
    "        mdp[\"states\"][-1][\"id\"] = s\n",
    "        mdp[\"states\"][-1][\"actions\"] = []\n",
    "        \n",
    "        for a in P[s]:\n",
    "            mdp[\"states\"][-1][\"actions\"].append({})\n",
    "            mdp[\"states\"][-1][\"actions\"][-1][\"id\"] = a\n",
    "            mdp[\"states\"][-1][\"actions\"][-1][\"transitions\"] = []\n",
    "            \n",
    "            for i, (prob, s_prime, reward, done) in enumerate(P[s][a]):\n",
    "                \n",
    "                mdp[\"states\"][-1][\"actions\"][-1][\"transitions\"].append({})\n",
    "                mdp[\"states\"][-1][\"actions\"][-1][\"transitions\"][-1][\"id\"] = i\n",
    "                mdp[\"states\"][-1][\"actions\"][-1][\"transitions\"][-1][\"probability\"] = prob\n",
    "                mdp[\"states\"][-1][\"actions\"][-1][\"transitions\"][-1][\"reward\"] = reward\n",
    "                mdp[\"states\"][-1][\"actions\"][-1][\"transitions\"][-1][\"to\"] = s_prime\n",
    "    return mdp\n",
    "\n",
    "def create_json_from_P(P, name, gamma=0.99, write=True):\n",
    "    \n",
    "    mdp = create_MDP(P, gamma)\n",
    "    js = json.dumps(mdp, ensure_ascii=False, sort_keys=True, indent=2)\n",
    "    if write:\n",
    "        with open(\"./\" + name + \".json\", \"w\") as text_file:\n",
    "            text_file.write(js)\n",
    "    return mdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_grid = [[' ',' ',' ',+100],\n",
    "            [' ','#',' ',-100],\n",
    "            ['@',' ',' ',' ']]\n",
    "\n",
    "def run_policy_iteration_from_T(T, gamma):\n",
    "    mdp = MDPToolBoxWrapper(create_MDP(T, gamma))\n",
    "    pi = mdptoolbox.mdp.PolicyIteration(mdp.transitions, mdp.rewards, mdp.gamma)\n",
    "    pi.run()\n",
    "    return pi\n",
    "\n",
    "def run_policy_iteration(mdp):\n",
    "    pi = mdptoolbox.mdp.PolicyIteration(mdp.transitions, mdp.rewards, mdp.gamma)\n",
    "    pi.run()\n",
    "    return pi\n",
    "    \n",
    "class GridWorld(object):\n",
    "    \"\"\"\n",
    "    This is a nice approximation for all the complexity of the entire universe \n",
    "        â€“Charles Isbell\n",
    "    \"\"\"\n",
    "    MARKER_WALL = '#'\n",
    "    MARKER_CURR_STATE = '@'\n",
    "    MARKER_FREE_SPACE = ' '\n",
    "    \n",
    "    def __init__(self, grid, living_reward=-0.01, gamma=0.99, action_noise_dist=[0.1, 0.8, 0.1]):\n",
    "        \n",
    "        \"\"\"\n",
    "        Returns a grid world with input specifications. \n",
    "        \n",
    "        Grid spec: '#' walls, '<int>' terminal state, 'S' initial position, ' ' free space\n",
    "        \"\"\"\n",
    "        \n",
    "        assert(len(grid) != 0 and len(grid[0]) != 0)\n",
    "        \n",
    "        self.grid = grid.copy()\n",
    "        self.width = len(grid[0])\n",
    "        self.height = len(grid)\n",
    "        self.living_reward = living_reward\n",
    "        self.action_noise_dist = action_noise_dist\n",
    "        self.actions_name = [\"North\", \"East\", \"South\", \"West\"]\n",
    "        self.actions = [(-1, 0), (0, 1), (1, 0), (0, -1)]\n",
    "        self.curr_state, self.states = self._compute_states(grid)\n",
    "        self.nA = len(self.actions)\n",
    "        self.nS = len(self.states)\n",
    "        self.states_to_idx = dict(zip(self.states, range(self.nS)))\n",
    "        self.T = self.get_T()\n",
    "        self.gamma = gamma\n",
    "        self.mdp = MDPToolBoxWrapper(create_MDP(self.T, self.gamma))\n",
    "        self.policy_iteration_results = run_policy_iteration(self.mdp)\n",
    "    \n",
    "    def _compute_states(self, grid, feature_type='2d_loc'):\n",
    "        \"\"\"\n",
    "        Returns: Computes number of states in the grid world.\n",
    "        Input: \n",
    "            grid: grid description as python 2d array of strings (Ref: book_grid)\n",
    "        \"\"\"\n",
    "        states = []\n",
    "        for row in range(self.height): #[::-1]: # all rows, reverse order so that 0th state is (0,0)\n",
    "            for col in range(self.width): # all cols\n",
    "                \n",
    "                # State feature is simply (row#, col#)\n",
    "                state_feat1, state_feat2 = row, col\n",
    "                # Walls act like obstacles, and bumping into them won't change state (for now)\n",
    "                if grid[row][col] != self.MARKER_WALL:\n",
    "                    if grid[row][col] == self.MARKER_CURR_STATE:\n",
    "                        if feature_type == '2d_loc':\n",
    "                            init_state = (state_feat1, state_feat2)\n",
    "                        else:\n",
    "                            raise ValueError(\"Not Supported feature_type={}!\".format(feature_type))\n",
    "                    states.append((state_feat1, state_feat2))\n",
    "        return init_state, states\n",
    "    \n",
    "    def _is_terminal(self, s_idx):\n",
    "        \"\"\"\n",
    "        Returns: boolean representing whether a state represented by its index is a terminal\n",
    "            As per current model, this will be True for grid cells with integer reward value\n",
    "        \"\"\"\n",
    "        r, c = self.states[s_idx]\n",
    "        return isinstance(self.grid[r][c], int) or isinstance(self.grid[r][c], float)\n",
    "    \n",
    "    def reward(self, s_idx):\n",
    "        r, c = self.states[s_idx]\n",
    "        if isinstance(self.grid[r][c], int) or isinstance(self.grid[r][c], float):\n",
    "            return float(self.grid[r][c])\n",
    "        else:\n",
    "            return self.living_reward\n",
    "        \n",
    "    def stochastic_action(self, a_idx):\n",
    "        return np.random.choice([a_idx-1, a_idx, (a_idx+1)%self.nA], p=self.action_noise_dist)\n",
    "    \n",
    "    def next_states_with_probs(self, s_idx, a_idx):\n",
    "        \"\"\"\n",
    "        Returns: Returns (next states, probabilities next states) for given state and stochastic action\n",
    "        \"\"\"\n",
    "        assert(len(self.action_noise_dist) == 3)\n",
    "        \n",
    "        state_primes_to_probs = {}\n",
    "        r, c = self.states[s_idx]\n",
    "        for j, noisy_a_idx in enumerate([a_idx-1, a_idx, (a_idx+1)%self.nA]): # hard coded for len(self.action_noise_dist) == 3\n",
    "            \n",
    "            dr, dc = self.actions[noisy_a_idx]\n",
    "            p = self.action_noise_dist[j]\n",
    "            new_r, new_c = r+dr, c+dc\n",
    "            if new_r < 0 or new_r >= self.height \\\n",
    "                or new_c < 0 or new_c >= self.width \\\n",
    "                or self.grid[new_r][new_c] == self.MARKER_WALL:\n",
    "                    new_r, new_c = r, c\n",
    "                    \n",
    "            if self.states_to_idx[(new_r, new_c)] in state_primes_to_probs:\n",
    "                state_primes_to_probs[self.states_to_idx[(new_r, new_c)]] += p\n",
    "            else:\n",
    "                state_primes_to_probs[self.states_to_idx[(new_r, new_c)]] = p\n",
    "            \n",
    "        return state_primes_to_probs.items()\n",
    "    \n",
    "    def get_T(self):\n",
    "        \"\"\"\n",
    "        Returns: MDP Transitions T{s}{a} = (p, s_prime, r, done) for all s, a, s_prime\n",
    "        \"\"\"\n",
    "        mdp = {}\n",
    "        for s_idx in range(self.nS):\n",
    "        \n",
    "            mdp[s_idx] = {}\n",
    "            for a_idx in range(self.nA):\n",
    "                if self._is_terminal(s_idx):\n",
    "                    mdp[s_idx][a_idx] = [(1., s_idx, 0., True)]\n",
    "                else:\n",
    "                    mdp[s_idx][a_idx] = []\n",
    "                    for k, (s_prime_idx, next_p) in enumerate(self.next_states_with_probs(s_idx, a_idx)):\n",
    "                        mdp[s_idx][a_idx].append((next_p, s_prime_idx, self.reward(s_prime_idx), False))\n",
    "        return mdp\n",
    "    \n",
    "    def get_optimal_policy(self):\n",
    "        return self.policy_iteration_results.policy\n",
    "        \n",
    "    def get_possible_actions(self, state):\n",
    "        \n",
    "        if self.__is_terminal(state):\n",
    "            return None\n",
    "        \n",
    "        return self.actions\n",
    "\n",
    "    def act(self, s_idx, a_idx):\n",
    "        \n",
    "        a_idx = self.stochastic_action(a_idx)\n",
    "        s_prime_idx = np.random.choice(range(self.nS), p=self.mdp.transitions[a_idx, s_idx])\n",
    "        return s_prime_idx\n",
    "    \n",
    "    def sample_trajectory(self):\n",
    "        \n",
    "        tau = []\n",
    "        Pi = self.get_optimal_policy()\n",
    "        s_idx = np.random.randint(self.nS)\n",
    "        \n",
    "        while True:\n",
    "            a_idx = Pi[s_idx]\n",
    "            tau.append((s_idx, a_idx))\n",
    "            if self._is_terminal(s_idx):\n",
    "                # Reached terminal state, terminate trajectory here\n",
    "                break\n",
    "            s_prime_idx = self.act(s_idx, a_idx)\n",
    "            s_idx = s_prime_idx\n",
    "            \n",
    "        return tau\n",
    "    \n",
    "    def interpret_trajectory(self, tau):\n",
    "        \n",
    "        for (s_idx, a_idx) in tau:\n",
    "            print(self.get_state_representation_2d(self.states[s_idx]))\n",
    "            print(self.actions_name[a_idx])\n",
    "    \n",
    "    def disp_custom_grid(self, state_values, formatting=lambda x: \"{:+.3f}\".format(x)):\n",
    "        \n",
    "        \"\"\"\n",
    "        Returns: state_values representation of states\n",
    "        Input: state_values associated with each state\n",
    "        \"\"\"\n",
    "        self.state_values_dict = {self.states[i]: state_values[i] for i in range(self.nS)}\n",
    "        \n",
    "        msg = ''\n",
    "        cell_filler = \"_\"\n",
    "        grid = self.grid\n",
    "        for r in range(self.height):\n",
    "            for c in range(self.width):\n",
    "                if grid[r][c] != self.MARKER_WALL:\n",
    "                    tt = formatting(self.state_values_dict[(r,c)])\n",
    "                else: # Values of unreachable states (Walls) are 0\n",
    "                    tt = formatting(0)\n",
    "                msg += tt #\"{txt:{fill}^5s}\".format(txt=tt, fill=cell_filler)\n",
    "                msg += \"\\t\"\n",
    "            msg += \"\\n\"\n",
    "        msg += \"\\n\"\n",
    "        print(msg)\n",
    "        \n",
    "    def get_state_representation_2d(self, state):\n",
    "        \n",
    "        msg = ''\n",
    "        cell_filler = \"_\"\n",
    "        grid = self.grid\n",
    "        curr_r, curr_c = state\n",
    "        for r in range(self.height):\n",
    "            for c in range(self.width):\n",
    "                if r == curr_r and c == curr_c:\n",
    "                    tt = self.MARKER_CURR_STATE\n",
    "                elif grid[r][c] == self.MARKER_FREE_SPACE\\\n",
    "                    or (grid[r][c] == self.MARKER_CURR_STATE and (r != curr_r or c != curr_c)):\n",
    "                    tt = cell_filler\n",
    "                else:\n",
    "                    if isinstance(grid[r][c], int) or isinstance(grid[r][c], float):\n",
    "                        tt = \"{:+d}\".format(grid[r][c])\n",
    "                    else:\n",
    "                        tt = grid[r][c]\n",
    "                msg += \"{txt:{fill}^5s}\".format(txt=tt, fill=cell_filler)\n",
    "                msg += \"\\t\"\n",
    "            msg += \"\\n\"\n",
    "        msg += \"\\n\"\n",
    "        return msg\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.get_state_representation_2d(self.curr_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw = GridWorld(book_grid, action_noise_dist=[0.,1.,0.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____\t_____\t_____\t+100_\t\n",
      "_____\t__#__\t_____\t-100_\t\n",
      "__@__\t_____\t_____\t_____\t\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(gw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States (fused into a single grid-bc it's possible to do so here):\n",
      "0\t1\t2\t3\t\n",
      "4\t0\t5\t6\t\n",
      "7\t8\t9\t10\t\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"States (fused into a single grid-bc it's possible to do so here):\")\n",
    "gw.disp_custom_grid(range(gw.nS), formatting=lambda x: str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 0, -0.01, False), (0.0, 1, -0.01, False)],\n",
       " 1: [(0.0, 0, -0.01, False), (1.0, 1, -0.01, False), (0.0, 4, -0.01, False)],\n",
       " 2: [(0.0, 1, -0.01, False), (1.0, 4, -0.01, False), (0.0, 0, -0.01, False)],\n",
       " 3: [(0.0, 4, -0.01, False), (1.0, 0, -0.01, False)]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(gw.T.keys())\n",
    "gw.T[0] # S X A X (P(s'), s', R(s'), Done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(mdp.transitions[:, :, :]) # A x S x S'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "East\tEast\tEast\tNorth\t\n",
      "North\tNorth\tNorth\tNorth\t\n",
      "North\tEast\tNorth\tWest\t\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gw.disp_custom_grid(gw.get_optimal_policy(), lambda x: \"{:}\".format(gw.actions_name[x]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(10, 3), (9, 0), (5, 0), (2, 1), (3, 0)]\n"
     ]
    }
   ],
   "source": [
    "Tau = gw.sample_trajectory()\n",
    "print(Tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____\t_____\t_____\t+100_\t\n",
      "_____\t__#__\t_____\t-100_\t\n",
      "_____\t_____\t_____\t__@__\t\n",
      "\n",
      "\n",
      "West\n",
      "_____\t_____\t_____\t+100_\t\n",
      "_____\t__#__\t_____\t-100_\t\n",
      "_____\t_____\t__@__\t_____\t\n",
      "\n",
      "\n",
      "North\n",
      "_____\t_____\t_____\t+100_\t\n",
      "_____\t__#__\t__@__\t-100_\t\n",
      "_____\t_____\t_____\t_____\t\n",
      "\n",
      "\n",
      "North\n",
      "_____\t_____\t__@__\t+100_\t\n",
      "_____\t__#__\t_____\t-100_\t\n",
      "_____\t_____\t_____\t_____\t\n",
      "\n",
      "\n",
      "East\n",
      "_____\t_____\t_____\t__@__\t\n",
      "_____\t__#__\t_____\t-100_\t\n",
      "_____\t_____\t_____\t_____\t\n",
      "\n",
      "\n",
      "North\n"
     ]
    }
   ],
   "source": [
    "gw.interpret_trajectory(Tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDP ToolBox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp_descr = create_MDP(gw.T, 0.99)\n",
    "mdp = MDPToolBoxWrapper(mdp_descr)\n",
    "\n",
    "initial_policy = np.random.choice(mdp.nA, size=mdp.nS)\n",
    "pi = mdptoolbox.mdp.PolicyIteration(\n",
    "    mdp.transitions,\n",
    "    mdp.rewards,\n",
    "    mdp.gamma,\n",
    "    policy0=initial_policy,\n",
    "    eval_type=0\n",
    ")\n",
    "# pi.setSilent()\n",
    "pi.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdp.transitions[0,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+97.990\t+98.990\t+100.000\t+0.000\t\n",
      "+97.000\t+0.000\t+98.990\t+0.000\t\n",
      "+96.020\t+97.000\t+97.990\t+97.000\t\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gw.disp_custom_grid(pi.V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "East\tEast\tEast\tNorth\t\n",
      "North\tNorth\tNorth\tNorth\t\n",
      "North\tEast\tNorth\tWest\t\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gw.disp_custom_grid(pi.policy, lambda x: \"{:}\".format(gw.actions_name[x]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDP Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydot\n",
    "import networkx as nx\n",
    "from networkx.drawing.nx_agraph import write_dot\n",
    "import tempfile\n",
    "\n",
    "def disp_mdp(mdp_js, V):\n",
    "    \n",
    "    tmp_img = tempfile.mktemp()\n",
    "    G=nx.DiGraph()\n",
    "    for s in mdp_js['states']:\n",
    "        for a in s['actions']:\n",
    "            for t in a['transitions']:\n",
    "                ecolor='red' if a['id'] else 'green'\n",
    "                elabel='p={}, r={}'.format(round(t['probability'],4), t['reward'])\n",
    "                G.add_edge(s['id'], t['to'],\n",
    "                        color=ecolor,\n",
    "                        label=elabel)\n",
    "    \n",
    "    mapping = {g: str(g) + \", \" + str(g) for g in G}\n",
    "    # print(mapping)\n",
    "    G = nx.relabel_nodes(G,  mapping)\n",
    "    # print(G.nodes())\n",
    "    pos=nx.fruchterman_reingold_layout(G)\n",
    "    # nx.draw_networkx_edges(G,pos,width=8,alpha=0.5,edge_color='b')\n",
    "    # nx.draw(G,with_labels=True)\n",
    "    write_dot(G, tmp_img)\n",
    "    g = pydot.graph_from_dot_file(tmp_img)[0]\n",
    "    g.write_png(tmp_img)\n",
    "    img = plt.imread(tmp_img)\n",
    "\n",
    "    plt.figure(figsize=(30,20))\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_mdp(mdp_descr, pi.V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "littman_irl_python3",
   "language": "python",
   "name": "littman_irl_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
